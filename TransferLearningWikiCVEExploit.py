import torch
from fastai import *
from fastai.text import *
from fastai.vision.all import *
from fastai.text.all import *
import pandas as pd
from sklearn.utils import shuffle
from sklearn.metrics import *
from sklearn.model_selection import train_test_split
import numpy as np
from torch.cuda import is_available
import torch


def train_language_model(lm_data_bunch, encoder=None, dropout=0.2, pre_trained=True, test_mode=False, metrics=[accuracy], weight_decay=0.1) :

    learn = language_model_learner(lm_data_bunch, AWD_LSTM, drop_mult=dropout, pretrained=pre_trained, metrics=metrics, wd=weight_decay)
    if encoder is not None: 
        learn.load_encoder(encoder)

    if not test_mode :
        learn.freeze()
        learn.fit_one_cycle(2, 1e-2)

        learn.freeze_to(-2)
        learn.fit_one_cycle(1, slice(1e-4, 1e-2))

        learn.freeze_to(-3)
        learn.fit_one_cycle(1, slice(1e-5, 5e-3))

        learn.unfreeze()
        learn.fit_one_cycle(10, 1e-3)
        learn.save_encoder('language_model_finetuned')
    else:
        learn.unfreeze()
        learn.fit_one_cycle(2, 1e-3)

    return learn

def train_classifier_model(class_data_bunch, encoder=None,dropout=0.5, pre_trained=True, test_mode=False, metrics=[accuracy]):

	learn_class = text_classifier_learner(class_data_bunch, AWD_LSTM, drop_mult=dropout, pretrained=pre_trained, metrics=metrics)

	if encoder is not None: 
	   learn_class = learn_class.load_encoder(encoder)

	if not test_mode:
		learn_class.freeze()
		learn_class.fit_one_cycle(4, slice(1e-2,2e-2))

		learn_class.freeze_to(-2)
		learn_class.fit_one_cycle(4, slice(1e-2/(2.6**4),1e-2))

		learn_class.freeze_to(-3)
		learn_class.fit_one_cycle(4, slice(5e-3/(2.6**4),5e-3))

		learn_class.unfreeze()
		learn_class.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
		learn_class.export('export.pkl')
        
        
	else:
		learn_class.unfreeze()  
		learn_class.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-3))
		learn_class.export('export.pkl')

	return learn_class


if __name__=='__main__':

    if torch.cuda.is_available():    
        device = torch.device("cuda")
        print(f"There are {torch.cuda.device_count()} GPU(s) available")
        print("GPU: ", torch.cuda.get_device_name(0))
    else:
        print('CUDA Not Available')    
    
    path = untar_data(URLs.WIKITEXT_TINY)
    df_wiki = pd.read_csv(path/'train.csv')
    ft_lm_nvd_path = 'nvd.csv'
    ft_cls_exploitdb_path = 'exploitdb.csv'
    df_lm_nvd = pd.read_csv(ft_lm_nvd_path, encoding = "ISO-8859-1", low_memory = False)
    df_cls_exploitdb = pd.read_csv(ft_cls_exploitdb_path)
    df_cls_exploitdb = shuffle(df_cls_exploitdb)
    df_cve_description = df_lm_nvd.iloc[:,2:3]
    df_exploit_descriptions = df_cls_exploitdb.iloc[:,2:3]
    df_exploit_labels = df_cls_exploitdb.iloc[:,5:6]
    df_exploit_labels.replace(['dos', 'local', 'remote', 'webapps'], [0,1,2,3],inplace=True)
    print(df_exploit_labels.sample(10))
    
    training_x, test_x, training_y, test_y = train_test_split(df_exploit_descriptions, df_exploit_labels, test_size = 0.2, random_state=2022,stratify=df_exploit_labels)
    
    dls_lm = TextDataLoaders.from_df(df_wiki, path, is_lm=True, valid_pct=0.2)
    dls_lm.show_batch(max_n=5)
    dls_lm_cve = TextDataLoaders.from_df(df_lm_nvd, path, is_lm=True, valid_pct=0.2, text_vocab= dls_lm.vocab)
    dls_lm_cve.show_batch(max_n=5)
    
    training_set = pd.concat([training_x,training_y], axis = 1)
    
    learn = train_language_model(dls_lm_cve)
    dls_clas = TextDataLoaders.from_df(training_set ,text_vocab=dls_lm_cve.vocab,label_col=1)
    learn_class = train_classifier_model(dls_clas, encoder='language_model_finetuned', pre_trained=True) 

    test_descs = test_x.iloc[:,0:1].values
    predictions = []
    for i in range(len(test_descs)):
        pred = learn_class.predict(test_descs[i,0])
        predictions.append(pred[1])
    print(predictions)
     
    true_labels = test_y
    rep = classification_report(true_labels, predictions, target_names =['dos', 'local', 'remote', 'webapps'])
    print(rep)

