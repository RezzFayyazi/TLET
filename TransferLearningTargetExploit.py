import torch
from fastai import *
from fastai.text import *
from fastai.vision.all import *
from fastai.text.all import *
import pandas as pd
from sklearn.utils import shuffle
from sklearn.metrics import *
from sklearn.model_selection import train_test_split
import numpy as np
from torch.cuda import is_available
import torch



def train_language_model(lm_data_bunch, encoder=None, dropout=0.2, pre_trained=True, test_mode=False, metrics=[accuracy], weight_decay=0.1) :

    learn = language_model_learner(lm_data_bunch, AWD_LSTM, drop_mult=dropout, pretrained=pre_trained, metrics=metrics, wd=weight_decay)
    if encoder is not None: 
        learn.load_encoder(encoder)

    if not test_mode :
        learn.freeze()
        learn.fit_one_cycle(2, 1e-2)

        learn.freeze_to(-2)
        learn.fit_one_cycle(1, slice(1e-4, 1e-2))

        learn.freeze_to(-3)
        learn.fit_one_cycle(1, slice(1e-5, 5e-3))

        learn.unfreeze()
        learn.fit_one_cycle(10, 1e-3)
        learn.save_encoder('language_model_exploitdb')
    else:
        learn.unfreeze()
        learn.fit_one_cycle(2, 1e-3)

    return learn

def train_classifier_model(class_data_bunch, encoder=None,dropout=0.5, pre_trained=True, test_mode=False, metrics=[accuracy]):

	learn_class = text_classifier_learner(class_data_bunch, AWD_LSTM, drop_mult=dropout, pretrained=pre_trained, metrics=metrics)

	if encoder is not None: 
	    learn_class.load_encoder(encoder)

	if not test_mode:
		#learn_class.freeze()
		learn_class.fit_one_cycle(4, slice(1e-2,2e-2))

		learn_class.freeze_to(-2)
		learn_class.fit_one_cycle(4, slice(1e-2/(2.6**4),1e-2))

		learn_class.freeze_to(-3)
		learn_class.fit_one_cycle(4, slice(5e-3/(2.6**4),5e-3))

		learn_class.unfreeze()
		learn_class.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
		learn_class.export('classifier_model_exploit_tl.pkl')
        
        
	else:
		learn_class.unfreeze()  
		learn_class.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-3))
		learn_class.export('classifier_model_cve_exploit.pkl')

	return learn_class


if __name__=='__main__':

    if not torch.cuda.is_available():    
        print('CUDA Not Available') 
    else:
    	torch.cuda.set_device(0)
    	torch.multiprocessing.freeze_support()

    ft_cls_exploitdb_path = 'C:/Users/pentium/Desktop/Projects/TLET/Datasets/exploitdb.csv'
    df_cls_exploitdb = pd.read_csv(ft_cls_exploitdb_path)
    df_cls_exploitdb = shuffle(df_cls_exploitdb)
    df_cls_exploitdb.columns = df_cls_exploitdb.columns.str.strip()
    path_exploit = 'C:/Users/pentium/Desktop/Projects/TLET/Encoders'
    df_exploit_descriptions = df_cls_exploitdb.iloc[:,2:3]
    print(df_exploit_descriptions)
    df_exploit_labels = df_cls_exploitdb.iloc[:,5:6]
    print(df_exploit_labels)
    
    
    df_exploit_labels.replace(['dos', 'local', 'remote', 'webapps'], [0,1,2,3],inplace=True)
    print(df_exploit_labels.sample(10))
    
    dls_lm = TextDataLoaders.from_df(df_exploit_descriptions, path_exploit, is_lm=True, valid_pct=0.1)
    dls_lm.show_batch(max_n=5)
    
    learn = train_language_model(dls_lm)

    training_x, test_x, training_y, test_y = train_test_split(df_exploit_descriptions, df_exploit_labels, test_size = 0.2, random_state=2022)

    training_set = pd.concat([training_x,training_y], axis = 1)
    print(training_set)    
    
    dls_clas = TextDataLoaders.from_df(training_set ,text_vocab=dls_lm.vocab)
    learn = train_classifier_model(dls_clas, encoder='language_model_exploitdb') 


    test_descs = test_x.iloc[:,0:1].values
    print(len(test_descs))
    predictions = []
    for i in range(len(test_descs)):
        pred = learn.predict(test_descs[i,0])
        predictions.append(pred[1])
    print(predictions)
     
    true_labels = test_y
    print(true_labels)

    rep = classification_report(true_labels, predictions, target_names =['dos', 'local', 'remote', 'webapps'])
    print(rep)

